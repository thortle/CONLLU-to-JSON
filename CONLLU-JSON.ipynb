{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Python/Algorithmique\n",
    "Ce programme traite des fichiers au format CONLLU pour générer des statistiques détaillées. Il inclut des fonctionnalités pour la déduplication des fragments de N-Grammes, l'extraction de motifs de fréquence, et la génération de Skip-Grammes, en répondant aux exigences spécifiques de la partie individuelle du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des bibliothèques nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de lecture du fichier CONLLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(file_name):\n",
    "    # On initialise une liste vide qui contiendra toutes les phrases\n",
    "    # et une liste temporaire pour stocker les lignes de la phrase courante\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    # Ouverture du fichier en lecture avec encodage UTF-8\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # On enlève les espaces et retours à la ligne superflus\n",
    "            line = line.strip()\n",
    "\n",
    "            # On ignore les lignes de commentaires\n",
    "            if line.startswith('# text'):\n",
    "                continue\n",
    "\n",
    "            if not line:\n",
    "                # Si on a des lignes dans la phrase courante,\n",
    "                # on ajoute la phrase à notre liste de phrases\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "\n",
    "            # Analyse la ligne et ajoute les infos pertinentes à la phrase courante si valide.\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 4:\n",
    "                    current_sentence.append({\n",
    "                        \"id\": parts[0],\n",
    "                        \"form\": parts[1],\n",
    "                        \"lemma\": parts[2],\n",
    "                        \"upos\": parts[3],\n",
    "                        \"deprel\": parts[7] if len(parts) > 7 else None\n",
    "                    })\n",
    "    # Pour ne pas oublier la dernière phrase si le fichier ne se termine pas par une ligne vide\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La fonction de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_conllu(sentences):\n",
    "    # On s'assure d'avoir un fichier CONLLU fiable\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if not all(key in token for key in [\"id\", \"form\", \"lemma\", \"upos\"]):\n",
    "                raise ValueError(\"CONLLU n'est pas trouvé ou est endommagé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traitement des Phrases\n",
    "On analyse les statistiques d'un ensemble de phrases au format CONLLU. La fonction prend en paramètre une liste de phrases (output de read_conllu) et retourne un hachage contenant les statistiques demandées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Analyse les statistiques d'un ensemble de phrases au format CONLLU.\n",
    "    Prend en paramètre une liste de phrases (output de read_conllu).\n",
    "    Retourne un hachage avec les statistiques demandées.\n",
    "    \"\"\"\n",
    "    # Initialisation des compteurs\n",
    "    nb_toks = 0\n",
    "    nb_sents = len(sentences)\n",
    "    forms = []\n",
    "    lemmas = []\n",
    "    nb_puncts = 0\n",
    "    pos_counts = defaultdict(Counter)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # On compte le nombre de tokens\n",
    "        nb_toks += len(sentence)\n",
    "        for token in sentence:\n",
    "            # On compte le nombre de ponctuations\n",
    "            if token[\"upos\"] == \"PUNCT\":\n",
    "                nb_puncts += 1\n",
    "                continue\n",
    "            # On récupère les formes, les lemmes et les POS\n",
    "            forms.append(token[\"form\"])\n",
    "            lemmas.append(token[\"lemma\"])\n",
    "            pos_counts[token[\"upos\"]][token[\"lemma\"]] += 1\n",
    "\n",
    "    # On compte le nombre de formes et de types\n",
    "    nb_forms = len(forms)\n",
    "    nb_types = len(set(forms))\n",
    "    # Calcul des moyennes\n",
    "    average_sent_length = nb_toks / nb_sents\n",
    "    average_form_length = sum(len(form) for form in forms) / nb_forms\n",
    "\n",
    "    # Création du hachage de retour\n",
    "    return {\n",
    "        \"nbToks\": nb_toks,\n",
    "        \"nbSents\": nb_sents,\n",
    "        \"nbForms\": nb_forms,\n",
    "        \"nbPuncts\": nb_puncts,\n",
    "        \"nbTypes\": nb_types,\n",
    "        \"averageSentLength\": average_sent_length,\n",
    "        \"averageFormLength\": average_form_length,\n",
    "        # Conversion des hachages en tableaux triés par fréquence\n",
    "        \"noun2freq\": sorted(pos_counts[\"NOUN\"].items(), key=lambda x: x[1], reverse=True),\n",
    "        \"verb2freq\": sorted(pos_counts[\"VERB\"].items(), key=lambda x: x[1], reverse=True),\n",
    "        \"adj2freq\": sorted(pos_counts[\"ADJ\"].items(), key=lambda x: x[1], reverse=True),\n",
    "        \"adv2freq\": sorted(pos_counts[\"ADV\"].items(), key=lambda x: x[1], reverse=True),\n",
    "        \"lem2freq\": sorted(Counter(lemmas).items(), key=lambda x: x[1], reverse=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Génération des N-Grammes\n",
    "- Filtrage des Lemmes\n",
    "- Calcul des N-Grammes\n",
    "- Tri par fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(lemmas, min_len=2, max_len=6):\n",
    "    # Calcul des n-grammes\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in [\",\", \".\", \"!\", \"?\", \":\", \";\", \"-\", \"_\", \"(\", \")\"]]\n",
    "\n",
    "    ngrams = defaultdict(list)\n",
    "    for n in range(min_len, max_len + 1):\n",
    "        for i in range(len(lemmas) - n + 1):\n",
    "            ngram = tuple(lemmas[i:i + n])\n",
    "            ngrams[n].append(ngram)\n",
    "    # Tri par fréquence\n",
    "    return {key: sorted(Counter(value).items(), key=lambda x: x[1], reverse=True) for key, value in ngrams.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dédoublenage des N-Grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprime les n-grammes plus courts redondants si leur fréquence est inférieure à un seuil.\n",
    "def deduplicate_ngrams(ngrams, seuil_dedoublonnage):\n",
    "    for n in sorted(ngrams.keys(), reverse=True):\n",
    "        if n - 1 not in ngrams:\n",
    "            continue\n",
    "        for longer_ngram, freq_longer in ngrams[n]:\n",
    "            shorter_candidates = [longer_ngram[:i] + longer_ngram[i + 1:] for i in range(len(longer_ngram))]\n",
    "            for shorter_ngram in shorter_candidates:\n",
    "                for idx, (short_ngram, freq_short) in enumerate(ngrams[n - 1]):\n",
    "                    if short_ngram == shorter_ngram and freq_short <= seuil_dedoublonnage * freq_longer:\n",
    "                        ngrams[n - 1].pop(idx)\n",
    "                        break\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction des Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrait et compte les fréquences des séquences de lemmes correspondant à des motifs POS donnés.\n",
    "def extract_patterns(sentences, patterns):\n",
    "    pattern_freqs = Counter()\n",
    "    for sentence in sentences:\n",
    "        lemmas = [token[\"lemma\"] for token in sentence]\n",
    "        pos_tags = [token[\"upos\"] for token in sentence]\n",
    "        for pattern in patterns:\n",
    "            pattern_len = len(pattern)\n",
    "            for i in range(len(lemmas) - pattern_len + 1):\n",
    "                if pos_tags[i:i + pattern_len] == pattern:\n",
    "                    pattern_freqs[\" \".join(lemmas[i:i + pattern_len])] += 1\n",
    "    return sorted(pattern_freqs.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Génération des Skip-Grammes à partir d'une liste des lemmes.\n",
    "Elle prend en compte un gap défini entre les mots et génère des séquences de longueur comprise entre min_len et max_len. Les skipgrammes sont ensuite triés par fréquence décroissante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgrams(lemmas, gap=1, min_len=2, max_len=3):\n",
    "    skipgrams = defaultdict(int)\n",
    "    n = len(lemmas)\n",
    "\n",
    "    for length in range(min_len, max_len + 1):\n",
    "        for i in range(n - length + 1):\n",
    "            skipgram = tuple(lemmas[i:i + length:gap])\n",
    "            skipgrams[skipgram] += 1\n",
    "\n",
    "    return sorted(skipgrams.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    " # SKIPGRAMS SANS PUNCT\n",
    "\n",
    " # def generate_skipgrams(lemmas, gap=1, min_len=2, max_len=3):\n",
    " #    lemmas = [lemma for lemma in lemmas if lemma not in [\",\", \".\", \"!\", \"?\", \":\", \";\", \"-\", \"_\", \"(\", \")\"]]\n",
    " #\n",
    " #    skipgrams = defaultdict(int)\n",
    " #    n = len(lemmas)\n",
    " #\n",
    " #    for length in range(min_len, max_len + 1):\n",
    " #        for i in range(n - length + 1):\n",
    " #            skipgram = tuple(lemmas[i:i + length:gap])\n",
    " #            skipgrams[skipgram] += 1\n",
    " #\n",
    " #    return sorted(skipgrams.items(), key=lambda x: x[1], reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(conllu_file, json_file, pattern_file=None, seuil_dedoublonnage=1.3, ngram_min=2, ngram_max=6, skipgram_gap=1):\n",
    "    # Lecture et validation du fichier CoNLL-U\n",
    "    sentences = read_conllu(conllu_file)\n",
    "    validate_conllu(sentences)\n",
    "    stats = process_sentences(sentences)\n",
    "\n",
    "    # Génération des n-grammes\n",
    "    lemmas = [token[\"lemma\"] for sentence in sentences for token in sentence]\n",
    "    ngrams = generate_ngrams(lemmas, ngram_min, ngram_max)\n",
    "    stats[\"ngrams\"] = deduplicate_ngrams(ngrams, seuil_dedoublonnage)\n",
    "\n",
    "    # Extraction des motifs si un fichier de motifs est fourni\n",
    "    if pattern_file:\n",
    "        try:\n",
    "            with open(pattern_file, 'r', encoding='utf-8') as pf:\n",
    "                patterns = json.load(pf)\n",
    "            stats[\"patterns\"] = extract_patterns(sentences, patterns)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Fichier de patterns '{pattern_file}' introuvable.\")\n",
    "\n",
    "    # Génération des skip-grammes\n",
    "    stats[\"skipgrams\"] = generate_skipgrams(lemmas, gap=skipgram_gap, min_len=ngram_min, max_len=ngram_max)\n",
    "\n",
    "    # Écriture des résultats dans un fichier JSON\n",
    "    with open(json_file, 'w', encoding='utf-8') as json_out:\n",
    "        json.dump(stats, json_out, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conllu_file_path = \"./fr_rhapsodie-ud-test.conllu\"\n",
    "    json_file_path = \"results.json\"\n",
    "    pattern_file_path = \"patterns.json\"\n",
    "\n",
    "    main(conllu_file_path, json_file_path, pattern_file=pattern_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSEUDO-CODE \n",
    "pour l'algorithme de calcul des index hiérarchiques des N-Grammes.\n",
    "\n",
    "L'algorithme a une complexité QUADRATIQUE. Elle est déterminée par les fonctions générerNgrammes et générerSkipgrammes, qui sont toutes deux de complexité O(k * m^2) où k est le nombre de longueurs de N-Grammes/SkipGrammes et m est le nombre de lemmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction lireConllu(fichier) {\n",
    "    liste phrases ← []\n",
    "    liste phraseCourante ← []\n",
    "    \n",
    "    tant que non finDeFichier(fichier) {\n",
    "        chaine ligne ← lireLigne(fichier)\n",
    "        ligne ← ligne.trim()\n",
    "        \n",
    "        si ligne.commence_par(\"# text\") {\n",
    "            continuer\n",
    "        }\n",
    "        \n",
    "        si ligne == \"\" {\n",
    "            si phraseCourante != [] {\n",
    "                phrases.ajouter(phraseCourante)\n",
    "                phraseCourante ← []\n",
    "            }\n",
    "        } sinon {\n",
    "            liste parties ← ligne.diviser(\"\\t\")\n",
    "            si parties.taille >= 4 {\n",
    "                dict token ← {\n",
    "                    \"id\": parties[0],\n",
    "                    \"form\": parties[1],\n",
    "                    \"lemma\": parties[2],\n",
    "                    \"upos\": parties[3],\n",
    "                    \"deprel\": parties[6]\n",
    "                }\n",
    "                phraseCourante.ajouter(token)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    si phraseCourante != [] {\n",
    "        phrases.ajouter(phraseCourante)\n",
    "    }\n",
    "    retourner phrases\n",
    "}\n",
    "\n",
    "fonction validerConllu(phrases) {\n",
    "    pour chaque phrase dans phrases {\n",
    "        pour chaque token dans phrase {\n",
    "            si non (token[\"id\"] et token[\"form\"] et token[\"lemma\"] et token[\"upos\"]) {\n",
    "                lever erreur(\"Le fichier CONLLU est incorrect ou incomplet\")\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fonction traiterPhrases(phrases) {\n",
    "    entier nbTokens ← 0\n",
    "    entier nbPhrases ← phrases.taille\n",
    "    ensemble formes ← {}\n",
    "    ensemble lemmes ← {}\n",
    "    entier ponctuations ← 0\n",
    "    dict comptePOS ← {}\n",
    "    \n",
    "    pour chaque phrase dans phrases {\n",
    "        nbTokens ← nbTokens + phrase.taille\n",
    "        pour chaque token dans phrase {\n",
    "            formes.ajouter(token[\"form\"])\n",
    "            lemmes.ajouter(token[\"lemma\"])\n",
    "            \n",
    "            si token[\"upos\"] == \"PUNCT\" {\n",
    "                ponctuations ← ponctuations + 1\n",
    "            } sinon {\n",
    "                comptePOS[token[\"upos\"]][token[\"lemma\"]] ← comptePOS[token[\"upos\"]][token[\"lemma\"]] + 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    dict stats ← {\n",
    "        \"nbTokens\": nbTokens,\n",
    "        \"nbPhrases\": nbPhrases,\n",
    "        \"nbFormes\": formes.taille,\n",
    "        \"nbTypes\": lemmes.taille,\n",
    "        \"ponctuations\": ponctuations,\n",
    "        \"posDist\": comptePOS\n",
    "    }\n",
    "    retourner stats\n",
    "}\n",
    "\n",
    "fonction générerNgrammes(lemmes, longueurMin, longueurMax) {\n",
    "    dict ngrammes ← {}\n",
    "    \n",
    "    pour entier n de longueurMin à longueurMax {\n",
    "        dict freqs ← {}\n",
    "        entier i ← 0\n",
    "        tant que i <= lemmes.taille - n {\n",
    "            liste ngram ← lemmes[i:i+n]\n",
    "            freqs[ngram] ← freqs[ngram] + 1\n",
    "            i ← i + 1\n",
    "        }\n",
    "        ngrammes[n] ← freqs\n",
    "    }\n",
    "    retourner ngrammes.trierParFrequence()\n",
    "}\n",
    "\n",
    "fonction dédupliquerNgrammes(ngrams, seuil) {\n",
    "    pour entier n de ngrams.taille à 2 par pas de -1 {\n",
    "        si non ngrams[n-1] {\n",
    "            continuer\n",
    "        }\n",
    "        pour chaque ngram dans ngrams[n] {\n",
    "            liste candidats ← générerSousNgrammes(ngram)\n",
    "            pour chaque candidat dans candidats {\n",
    "                si ngrams[n-1][candidat] < seuil {\n",
    "                    supprimer ngrams[n-1][candidat]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    retourner ngrams\n",
    "}\n",
    "\n",
    "fonction générerSkipgrammes(lemmes, intervalle, longueurMin, longueurMax) {\n",
    "    dict skipgrammes ← {}\n",
    "    \n",
    "    pour entier n de longueurMin à longueurMax {\n",
    "        dict freqs ← {}\n",
    "        entier i ← 0\n",
    "        tant que i <= lemmes.taille - n {\n",
    "            entier gap ← 0\n",
    "            tant que gap <= intervalle {\n",
    "                liste skip ← extraireSkipgramme(lemmes, i, n, gap)\n",
    "                freqs[skip] ← freqs[skip] + 1\n",
    "                gap ← gap + 1\n",
    "            }\n",
    "            i ← i + 1\n",
    "        }\n",
    "        skipgrammes[n] ← freqs\n",
    "    }\n",
    "    retourner skipgrammes.trierParFrequence()\n",
    "}\n",
    "\n",
    "fonction principale(fichierConllu, fichierJson, fichierMotifs, seuil, ngramMin, ngramMax, intervalle) {\n",
    "    liste phrases ← lireConllu(fichierConllu)\n",
    "    validerConllu(phrases)\n",
    "    \n",
    "    dict statistiques ← traiterPhrases(phrases)\n",
    "    liste lemmes ← extraireTousLemmes(phrases)\n",
    "    \n",
    "    dict ngrammes ← générerNgrammes(lemmes, ngramMin, ngramMax)\n",
    "    statistiques[\"ngrams\"] ← dédupliquerNgrammes(ngrammes, seuil)\n",
    "    \n",
    "    si fichierMotifs {\n",
    "        liste motifs ← chargerMotifs(fichierMotifs)\n",
    "        statistiques[\"patterns\"] ← extraireMotifs(phrases, motifs)\n",
    "    }\n",
    "    \n",
    "    statistiques[\"skipgrams\"] ← générerSkipgrammes(lemmes, intervalle, ngramMin, ngramMax)\n",
    "    écrireFichierJson(fichierJson, statistiques)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
